import streamlit as st
import google.generativeai as genai
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_core.messages import AIMessage, HumanMessage

# Configuration de l'API Gemini
genai.configure(api_key="AIzaSyCwWitJOAQDe8jsogTiPmep5ToOw_Vl-Rk")

# Connexion automatique √† la base de donn√©es SQLite
DB_PATH = "call_center_full_extended.db"
db = SQLDatabase.from_uri(f"sqlite:///{DB_PATH}")

def get_schema(db):
    return db.get_table_info()

def format_chat_history(chat_history):
    formatted_history = []
    for message in chat_history:
        if isinstance(message, AIMessage) and "Bonjour" in message.content:
            continue
        role = "Utilisateur" if isinstance(message, HumanMessage) else "Assistant"
        formatted_history.append(f"{role}: {message.content}")
    return "\n".join(formatted_history[-5:])

def get_sql_chain(schema, chat_history, question):
    template = """
    Vous √™tes un expert SQL. Convertissez cette question en requ√™te SQL pr√©cise.
    Sch√©ma : {schema}
    Historique : {chat_history}
    Question : {question}
    Requ√™te SQL :
    """
    
    formatted_history = format_chat_history(chat_history)
    
    return template.format(
        schema=schema, 
        chat_history=formatted_history, 
        question=question
    )

def get_nl_response(sql_query, schema, sql_response):
    template = """
    Transformer le r√©sultat SQL en r√©ponse claire en fran√ßais.
    Sch√©ma : {schema}
    Requ√™te : {sql_query}
    R√©sultat : {sql_response}
    R√©ponse: 
    """
    return template.format(sql_query=sql_query, schema=schema, sql_response=sql_response)

# Initialisation de l'historique
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage("Bonjour ! Je suis votre assistant SQL."),
    ]

def get_gemini_response(question, prompt, max_retries=3):
    for _ in range(max_retries):
        try:
            model = genai.GenerativeModel('gemini-pro')
            response = model.generate_content([prompt, question])
            return response.text.strip()
        except Exception as e:
            st.warning(f"Tentative √©chou√©e : {e}")
    return "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse."

def safe_add_to_history(chat_history, message):
    """Ajoute un message √† l'historique de mani√®re s√©curis√©e"""
    if message and isinstance(message, str) and message.strip():
        chat_history.append(AIMessage(content=message))
    elif not message:
        chat_history.append(AIMessage(content="Je n'ai pas pu g√©n√©rer de r√©ponse."))

# Configuration Streamlit
st.set_page_config(page_title="Assistant SQL", page_icon="üí¨")
st.header("Interrogez Votre Base de Donn√©es")

# R√©cup√©ration du sch√©ma
schema = get_schema(db)

# Affichage de l'historique
for message in st.session_state.chat_history:
    with st.chat_message("ai" if isinstance(message, AIMessage) else "human"):
        st.markdown(message.content)

# Champ de saisie
typing_user_query = st.chat_input("Posez votre question...")

if typing_user_query and typing_user_query.strip() != "":
    # Ajouter la question de l'utilisateur
    st.session_state.chat_history.append(HumanMessage(content=typing_user_query))
    
    with st.chat_message("human"):
        st.markdown(typing_user_query)
    
    with st.chat_message("ai"):
        # G√©n√©rer la requ√™te SQL
        prompt = get_sql_chain(schema, st.session_state.chat_history, typing_user_query)
        sql_query = get_gemini_response(typing_user_query, prompt)
        
        if sql_query:
            try:
                # Nettoyer la requ√™te SQL
                sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
                
                # Ex√©cuter la requ√™te
                sql_response = db.run(sql_query)
                
                # G√©n√©rer la r√©ponse en langage naturel
                response_prompt = get_nl_response(sql_query, schema, sql_response)
                response = get_gemini_response(typing_user_query, response_prompt)
                
                st.markdown(response)
                # Ajouter la r√©ponse √† l'historique de mani√®re s√©curis√©e
                safe_add_to_history(st.session_state.chat_history, response)
                
            except Exception as e:
                error_response = f"D√©sol√©, je n'ai pas pu ex√©cuter la requ√™te SQL, car la question n'est pas claire ou les donn√©es ne sont pas disponibles."
                st.markdown(error_response)
                safe_add_to_history(st.session_state.chat_history, error_response)
        else:
            error_response = "D√©sol√©, je n'ai pas pu g√©n√©rer de requ√™te SQL valide."
            st.markdown(error_response)
            safe_add_to_history(st.session_state.chat_history, error_response)
