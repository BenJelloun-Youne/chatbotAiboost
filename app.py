import streamlit as st
import google.generativeai as genai
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_core.messages import AIMessage, HumanMessage
import pandas as pd
import re
import os

# Configuration de l'API Gemini avec la version correcte du mod√®le
genai.configure(api_key="AIzaSyCwWitJOAQDe8jsogTiPmep5ToOw_Vl-Rk")

# Liste des mod√®les disponibles pour v√©rification
def list_available_models():
    try:
        models = genai.list_models()
        available_models = [model.name for model in models]
        return available_models
    except Exception as e:
        return f"Erreur lors de la liste des mod√®les: {str(e)}"

# Connexion √† la base de donn√©es SQLite
DB_PATH = "call_center_full_extended.db"
db = SQLDatabase.from_uri(f"sqlite:///{DB_PATH}")

def get_schema(db):
    """R√©cup√®re les informations de sch√©ma de la base de donn√©es."""
    return db.get_table_info()

def format_chat_history(chat_history, max_messages=5):
    """Formate l'historique de chat pour l'inclure dans le prompt."""
    formatted_history = []
    filtered_history = [msg for msg in chat_history 
                       if not (isinstance(msg, AIMessage) and "Bonjour" in msg.content)]
    
    for message in filtered_history[-max_messages:]:
        role = "Utilisateur" if isinstance(message, HumanMessage) else "Assistant"
        formatted_history.append(f"{role}: {message.content}")
    
    return "\n".join(formatted_history)

def execute_sql_query(query):
    """Ex√©cute une requ√™te SQL et g√®re les exceptions."""
    try:
        # Nettoyage de la requ√™te
        clean_query = re.sub(r'```sql|```', '', query).strip()
        
        # Ex√©cution de la requ√™te
        result = db.run(clean_query)
        
        # Si le r√©sultat est vide, retourner un message appropri√©
        if not result or result.strip() == "":
            return "Aucun r√©sultat trouv√© pour cette requ√™te."
        
        return result
    except Exception as e:
        st.error(f"Erreur lors de l'ex√©cution de la requ√™te SQL: {str(e)}")
        return f"Erreur: {str(e)}"

def get_gemini_response(prompt, max_retries=3):
    """Obtient une r√©ponse du mod√®le Gemini avec le mod√®le adapt√©."""
    # Utilisation du mod√®le gemini-1.0-pro √† la place de gemini-pro
    model_name = "gemini-1.5-pro"  # Version la plus r√©cente
    fallback_models = ["gemini-1.0-pro", "gemini-pro-vision"]  # Mod√®les de secours
    
    for attempt in range(max_retries):
        try:
            # Essayer d'abord avec le mod√®le principal
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            st.warning(f"Tentative {attempt+1} avec {model_name} √©chou√©e: {e}")
            
            # Si c'est une erreur de mod√®le non trouv√©, essayer avec les mod√®les de secours
            if "not found" in str(e).lower() and attempt < len(fallback_models):
                try:
                    backup_model = genai.GenerativeModel(fallback_models[attempt])
                    response = backup_model.generate_content(prompt)
                    # Si cela fonctionne, m√©moriser ce mod√®le pour les prochains appels
                    model_name = fallback_models[attempt]
                    return response.text.strip()
                except Exception as backup_error:
                    st.warning(f"Mod√®le de secours {fallback_models[attempt]} √©chou√©: {backup_error}")
            
            # Si c'est la derni√®re tentative, renvoyer une erreur
            if attempt == max_retries - 1:
                # Essayer une derni√®re tentative avec un mod√®le basique comme fallback final
                try:
                    basic_model = genai.GenerativeModel("text-bison@001")
                    response = basic_model.generate_content(prompt)
                    return response.text.strip()
                except:
                    return f"Erreur: Impossible d'acc√©der aux mod√®les Gemini. V√©rifiez votre cl√© API et les mod√®les disponibles."

def get_sql_prompt(schema, chat_history, question):
    """Cr√©e le prompt pour la g√©n√©ration de requ√™te SQL."""
    template = """
    Vous √™tes un expert SQL. Convertissez cette question en requ√™te SQL pr√©cise.
    
    Sch√©ma de la base de donn√©es:
    {schema}
    
    Historique des conversations:
    {chat_history}
    
    Question utilisateur: {question}
    
    R√©pondez uniquement avec la requ√™te SQL sans aucune explication ou texte suppl√©mentaire.
    """
    
    return template.format(
        schema=schema,
        chat_history=format_chat_history(chat_history),
        question=question
    )

def get_nl_response_prompt(schema, question, sql_query, sql_result):
    """Cr√©e le prompt pour la g√©n√©ration de r√©ponse en langage naturel."""
    template = """
    Transformer le r√©sultat SQL en r√©ponse claire en fran√ßais pour l'utilisateur.
    
    Sch√©ma de la base de donn√©es:
    {schema}
    
    Question originale: {question}
    
    Requ√™te SQL ex√©cut√©e:
    {sql_query}
    
    R√©sultat de la requ√™te:
    {sql_result}
    
    R√©pondez comme un assistant amical, en fran√ßais, en expliquant les r√©sultats de mani√®re claire.
    Si les r√©sultats sont vides ou nuls, expliquez pourquoi la requ√™te n'a peut-√™tre pas donn√© de r√©sultats.
    """
    
    return template.format(
        schema=schema,
        question=question,
        sql_query=sql_query,
        sql_result=sql_result
    )

def display_sql_result_as_table(result):
    """Affiche le r√©sultat SQL sous forme de tableau si possible."""
    try:
        # Tenter de convertir le r√©sultat en DataFrame
        if isinstance(result, str):
            # Analyser le r√©sultat sous forme de cha√Æne en lignes et colonnes
            lines = result.strip().split('\n')
            if len(lines) > 1:
                header = lines[0].split(',')
                data = [line.split(',') for line in lines[1:]]
                df = pd.DataFrame(data, columns=header)
                return df
        return None
    except Exception:
        return None

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Assistants KPIs et DATA", 
    page_icon="üìä", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# Stylisation et UI
st.markdown("""
<style>
    .main {
        background-color: #f5f7f9;
    }
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .chat-container {
        background-color: white;
        border-radius: 10px;
        padding: 20px;
        margin-top: 20px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .stTitle {
        color: #1E3A8A;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# En-t√™te
st.title("üìä Assistants KPIs et DATA")
st.markdown("Explorez vos donn√©es et KPIs du centre d'appels avec des questions en langage naturel.")

# V√©rification des mod√®les disponibles (√† ex√©cuter une seule fois)
if 'available_models_checked' not in st.session_state:
    available_models = list_available_models()
    st.session_state.available_models = available_models
    st.session_state.available_models_checked = True

# Barre lat√©rale avec informations sur le sch√©ma
with st.sidebar:
    st.header("Informations sur la base de donn√©es")
    
    # R√©cup√©ration et affichage du sch√©ma
    schema_info = get_schema(db)
    with st.expander("Sch√©ma de la base de donn√©es", expanded=False):
        st.code(schema_info, language="sql")
    
    # Options avanc√©es
    st.subheader("Options")
    show_sql = st.checkbox("Afficher les requ√™tes SQL", value=True)
    show_results_as_table = st.checkbox("Afficher les r√©sultats sous forme de tableau", value=True)
    
    # Afficher les mod√®les disponibles
    with st.expander("Mod√®les Gemini disponibles", expanded=False):
        if isinstance(st.session_state.available_models, list):
            for model in st.session_state.available_models:
                st.write(f"- {model}")
        else:
            st.write(st.session_state.available_models)
    
    # √Ä propos
    st.markdown("---")
    st.markdown("### √Ä propos")
    st.markdown("Cet assistant vous permet d'interroger votre base de donn√©es via des questions en langage naturel pour analyser vos KPIs et donn√©es.")

# Initialisation de l'historique
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage("Bonjour ! Je suis votre assistant KPIs et DATA. Comment puis-je vous aider √† analyser vos donn√©es aujourd'hui ?"),
    ]

# Affichage de l'historique du chat
with st.container():
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    for message in st.session_state.chat_history:
        with st.chat_message("ai" if isinstance(message, AIMessage) else "human"):
            st.markdown(message.content)
    
    st.markdown('</div>', unsafe_allow_html=True)

# Input utilisateur
user_query = st.chat_input("Posez votre question sur les KPIs ou les donn√©es...")

# Traitement de la requ√™te
if user_query:
    # Ajouter la question de l'utilisateur √† l'historique
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    # Afficher la question
    with st.chat_message("human"):
        st.markdown(user_query)
    
    # Traiter la question et afficher la r√©ponse
    with st.chat_message("ai"):
        with st.spinner("Analyse de votre question..."):
            # 1. R√©cup√©rer le sch√©ma
            schema = get_schema(db)
            
            # 2. G√©n√©rer la requ√™te SQL
            sql_prompt = get_sql_prompt(schema, st.session_state.chat_history, user_query)
            sql_query = get_gemini_response(sql_prompt)
            
            if sql_query and not sql_query.startswith("Erreur:"):
                # Afficher la requ√™te SQL si demand√©
                if show_sql:
                    st.markdown("**Requ√™te SQL g√©n√©r√©e:**")
                    st.code(sql_query, language="sql")
                
                # 3. Ex√©cuter la requ√™te
                sql_result = execute_sql_query(sql_query)
                
                # Essayer d'afficher les r√©sultats sous forme de tableau
                if show_results_as_table:
                    df = display_sql_result_as_table(sql_result)
                    if df is not None and not df.empty:
                        st.markdown("**R√©sultats:**")
                        st.dataframe(df, use_container_width=True)
                
                # 4. G√©n√©rer la r√©ponse en langage naturel
                nl_prompt = get_nl_response_prompt(schema, user_query, sql_query, sql_result)
                response = get_gemini_response(nl_prompt)
                
                # 5. Afficher la r√©ponse
                st.markdown("**Analyse:**")
                st.markdown(response)
                
                # 6. Ajouter la r√©ponse √† l'historique
                st.session_state.chat_history.append(AIMessage(content=response))
            else:
                error_msg = "Je n'ai pas pu g√©n√©rer une requ√™te SQL valide pour votre question. Pourriez-vous reformuler votre question ou fournir plus de d√©tails ?"
                if sql_query.startswith("Erreur:"):
                    error_msg = sql_query
                st.markdown(error_msg)
                st.session_state.chat_history.append(AIMessage(content=error_msg))
